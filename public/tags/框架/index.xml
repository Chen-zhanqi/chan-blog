<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>框架 - Tag - 辰深-技术与生活</title>
        <link>http://localhost:1313/tags/%E6%A1%86%E6%9E%B6/</link>
        <description>框架 - Tag - 辰深-技术与生活</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>czq181020@gmail.com (辰深)</managingEditor>
            <webMaster>czq181020@gmail.com (辰深)</webMaster><lastBuildDate>Thu, 25 Jun 2020 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/tags/%E6%A1%86%E6%9E%B6/" rel="self" type="application/rss+xml" /><item>
    <title>Django服务器端跨域</title>
    <link>http://localhost:1313/posts/2020/20200625_django%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E8%B7%A8%E5%9F%9F/</link>
    <pubDate>Thu, 25 Jun 2020 00:00:00 &#43;0000</pubDate>
    <author>辰深</author>
    <guid>http://localhost:1313/posts/2020/20200625_django%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E8%B7%A8%E5%9F%9F/</guid>
    <description><![CDATA[:::tip 服务器端解决跨域问题: django-cors-headers ::: 文档 django-cors-headers 安装 pip install -i https://pypi.douban.com/simple django-cors-headers 配置 settings.py 添加到已安装的应用程序中： INSTALLED_APPS = [ ... &#39;corsheaders&#39;， ... ] 确保]]></description>
</item>
<item>
    <title>DRF 过滤器组件</title>
    <link>http://localhost:1313/posts/2020/20200622_drf%E8%BF%87%E6%BB%A4%E5%99%A8%E7%BB%84%E4%BB%B6/</link>
    <pubDate>Mon, 22 Jun 2020 00:00:00 &#43;0000</pubDate>
    <author>辰深</author>
    <guid>http://localhost:1313/posts/2020/20200622_drf%E8%BF%87%E6%BB%A4%E5%99%A8%E7%BB%84%E4%BB%B6/</guid>
    <description><![CDATA[:::tip Django REST Framework. Filters ::: 基本信息 源码 rest_framework.filters 官方文档 DRF API Guide-Filtering django-filter DRF 过滤组件 DRF 搜索过滤组件 urls.py from django.urls import path from . import views urlpatterns = [ path(&#39;cars/&#39;, views.CarListAPIView.as_view()), # SearchFilter ] views.py # drf的SearchFilter # 第]]></description>
</item>
<item>
    <title>DRF 通用视图-视图工具类-视图集以及路由</title>
    <link>http://localhost:1313/posts/2020/20200621_drf%E9%80%9A%E7%94%A8%E8%A7%86%E5%9B%BE-%E8%A7%86%E5%9B%BE%E5%B7%A5%E5%85%B7%E7%B1%BB-%E8%A7%86%E5%9B%BE%E9%9B%86%E4%BB%A5%E5%8F%8A%E8%B7%AF%E7%94%B1/</link>
    <pubDate>Sun, 21 Jun 2020 00:00:00 &#43;0000</pubDate>
    <author>辰深</author>
    <guid>http://localhost:1313/posts/2020/20200621_drf%E9%80%9A%E7%94%A8%E8%A7%86%E5%9B%BE-%E8%A7%86%E5%9B%BE%E5%B7%A5%E5%85%B7%E7%B1%BB-%E8%A7%86%E5%9B%BE%E9%9B%86%E4%BB%A5%E5%8F%8A%E8%B7%AF%E7%94%B1/</guid>
    <description><![CDATA[:::tip Django REST Framework. Generic Views, MixIns, ViewSets and Routers ::: 基本信息 源码仓库 rest_framework.views rest_framework.generics rest_framework.viewsets rest_framework.routers 官方文档 API Guide-Generic views API Guide-ViewSets API Guide-Routers 关系图 ::: details View继承图与View图谱 ::: AIPView-API视图类 APIView 是]]></description>
</item>
<item>
    <title>Scrapy同时运行多个爬虫</title>
    <link>http://localhost:1313/posts/2020/20200519_scrapy%E5%90%8C%E6%97%B6%E8%BF%90%E8%A1%8C%E5%A4%9A%E4%B8%AA%E7%88%AC%E8%99%AB/</link>
    <pubDate>Tue, 19 May 2020 00:00:00 &#43;0000</pubDate>
    <author>辰深</author>
    <guid>http://localhost:1313/posts/2020/20200519_scrapy%E5%90%8C%E6%97%B6%E8%BF%90%E8%A1%8C%E5%A4%9A%E4%B8%AA%E7%88%AC%E8%99%AB/</guid>
    <description><![CDATA[多种实现方案: 开启多个命令行，分别执行scrapy cralw xxxx 编写脚本，执行工程下的所有爬虫 #!/usr/bin/env python # -*- coding:utf-8 -*- &#34;&#34;&#34; @file: run.py @time: 2020/05/19 9:49 @desc: None @Author: Chenzq @Wechat: * @contact: czq181020@gmail.com &#34;&#34;&#34; from scrapy.utils.project import get_project_settings from scrapy.crawler import]]></description>
</item>
<item>
    <title>重载Scrapy下载中间件及Cookie池</title>
    <link>http://localhost:1313/posts/2020/20200515_%E9%87%8D%E5%86%99scrapy%E4%B8%8B%E8%BD%BD%E4%B8%AD%E9%97%B4%E4%BB%B6/</link>
    <pubDate>Fri, 15 May 2020 00:00:00 &#43;0000</pubDate>
    <author>辰深</author>
    <guid>http://localhost:1313/posts/2020/20200515_%E9%87%8D%E5%86%99scrapy%E4%B8%8B%E8%BD%BD%E4%B8%AD%E9%97%B4%E4%BB%B6/</guid>
    <description><![CDATA[:::tip scrapy中间件重构 ::: 官方文档：Downloader Middleware 重写 import json # 处理json的包 import redis # Python操作redis的包 import random # 随机选择 from .useragent import]]></description>
</item>
<item>
    <title>Scrapy-Splash的使用</title>
    <link>http://localhost:1313/posts/2020/20200409_scrapy-splash%E7%9A%84%E4%BD%BF%E7%94%A8/</link>
    <pubDate>Thu, 09 Apr 2020 00:00:00 &#43;0000</pubDate>
    <author>辰深</author>
    <guid>http://localhost:1313/posts/2020/20200409_scrapy-splash%E7%9A%84%E4%BD%BF%E7%94%A8/</guid>
    <description><![CDATA[scrapy-splash的介绍 Scrapy没有JS engine, 无法爬取JavaScript生成的动态网页，只能爬取静态网页，而在现代的网络世界中，大]]></description>
</item>
<item>
    <title>基于Python的Tornado框架入门记录</title>
    <link>http://localhost:1313/posts/2020/20200405_%E5%9F%BA%E4%BA%8Epython%E7%9A%84tornado%E6%A1%86%E6%9E%B6%E5%85%A5%E9%97%A8/</link>
    <pubDate>Sun, 05 Apr 2020 00:00:00 &#43;0000</pubDate>
    <author>辰深</author>
    <guid>http://localhost:1313/posts/2020/20200405_%E5%9F%BA%E4%BA%8Epython%E7%9A%84tornado%E6%A1%86%E6%9E%B6%E5%85%A5%E9%97%A8/</guid>
    <description><![CDATA[Tornado简介 Tornado-基于Python的web服务端框架， 与现有主流的web服务端（以及大多数Python框架）有着明显的区别：]]></description>
</item>
<item>
    <title>某考研查询微信小程序爬虫</title>
    <link>http://localhost:1313/posts/2019/20190620_xxx%E8%80%83%E7%A0%94%E6%9F%A5%E8%AF%A2%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E7%88%AC%E8%99%AB/</link>
    <pubDate>Thu, 20 Jun 2019 00:00:00 &#43;0000</pubDate>
    <author>辰深</author>
    <guid>http://localhost:1313/posts/2019/20190620_xxx%E8%80%83%E7%A0%94%E6%9F%A5%E8%AF%A2%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E7%88%AC%E8%99%AB/</guid>
    <description><![CDATA[:::tip 解决某考研查询某小程序jwt权限认证与采集 ::: 前言 日常采集数据时, 有很多需要登录才能获取的数据, 不能解决权限认证这个问题, 就无法获取到想要采]]></description>
</item>
<item>
    <title>Scrapy process_item方法写入数据库记录</title>
    <link>http://localhost:1313/posts/2019/20190401_scrapy-process_item%E6%96%B9%E6%B3%95%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%B0%E5%BD%95/</link>
    <pubDate>Mon, 01 Apr 2019 00:00:00 &#43;0000</pubDate>
    <author>辰深</author>
    <guid>http://localhost:1313/posts/2019/20190401_scrapy-process_item%E6%96%B9%E6%B3%95%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%B0%E5%BD%95/</guid>
    <description><![CDATA[pipelines.py class MysqlPipeline(): def __init__(self, host, database, user, password, port): self.host = host self.database = database self.user = user self.password = password self.port = port @classmethod def from_crawler(cls, crawler): return cls( host=crawler.settings.get(&#39;MYSQL_HOST&#39;), database=crawler.settings.get(&#39;MYSQL_DATABASE&#39;), user=crawler.settings.get(&#39;MYSQL_USER&#39;), password=crawler.settings.get(&#39;MYSQL_PASSWORD&#39;), port=crawler.settings.get(&#39;MYSQL_PORT&#39;), ) def open_spider(self, spider): self.db = pymysql.connect(self.host, self.user, self.password, self.database, charset=&#39;utf8&#39;, port=self.port) self.cursor = self.db.cursor() def close_spider(self, spider): self.db.close() def process_item(self, item, spider): data = dict(item) # data[&#39;imgs&#39;] = data[&#39;image_paths&#39;][0] # del data[&#39;image_paths&#39;] com_id]]></description>
</item>
<item>
    <title>Flask全局异常处理</title>
    <link>http://localhost:1313/posts/2019/20190315_flask%E5%85%A8%E5%B1%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</link>
    <pubDate>Fri, 15 Mar 2019 00:00:00 &#43;0000</pubDate>
    <author>辰深</author>
    <guid>http://localhost:1313/posts/2019/20190315_flask%E5%85%A8%E5%B1%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</guid>
    <description><![CDATA[:::tip Flask自定义全局响应类 ::: api的设计中, 无论异常还是正常数据均需要服务器以json的格式返回, 为了对异常的统一管理, 同时为了后续更加方]]></description>
</item>
</channel>
</rss>
